### Accelerated Smoothing
The Accelerated Smoothing project involves sampling a dataset for training a surrogate model given a base classifier and a certain sigma value. This is achieved using the sample_data.py script, which takes arguments for the dataset name, path to the base classifier model, sigma value, and optional parameters such as batch size, skip value, split type, and number of samples (N). The sampled dataset is stored under a folder named sampled_dataset/.

The train_surrogate.py script trains a surrogate model on the sampled dataset using a custom loss function based on Jensen-Shannon divergence. It evaluates the trained model's performance by calculating the average certified radius (ACR) for each epoch using a confidence bound approach. The script saves the trained model, ACR values, training accuracy, ACR, and loss values for each epoch, and plots training accuracy, ACR, and loss curves.

```
python sample_data.py <dataset_name> <path_to_base_classifier_model> <sigma_value> --batch <batch_size> --skip <skip_value> --split <split_type> --N <N_value>
python train_surrogate.py <dataset_name> <model_type> <sigma_value> <data_path> --split <split_type> --batch <batch_size> --N <N_value>
```

Replace <dataset_name>, <path_to_base_classifier_model>, <sigma_value>, <model_type>, <data_path>, and other arguments as needed for your specific dataset and model configuration. These commands will run the respective scripts and perform the specified tasks for the Accelerated Smoothing project. Additionally, other metrics included in the main paper can be easily generated by tables.py and metrics.py scripts.

For our experiments we have used base models as the pretrained models provided by the Authors of "Certified Adversarial Robustness By Randomized Smoothing", these models can be downloaded from [here](https://drive.google.com/file/d/1h_TpbXm5haY5f-l4--IKylmdz6tvPoR4/view). 

We have included the sampled Class Counts in the supplementary materials. It's important to note that the x.pth and labels.pth files that we generate at each sampling will have the same indexing for all the cases irrespective of sigma and base model. So, to create the corresponding x.pth (referring to images) and labels.pth files, you can simply run sample_data.py with N = 1. This will generate the x.pth files and labels.pth files, which can be easily used alongside the class counts for any sigma value.